{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from threading import Thread\n",
    "# import importlib.util\n",
    "from PIL import Image\n",
    "import math\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "from edgetpu import __version__ as edgetpu_version\n",
    "assert parse_version(edgetpu_version) >= parse_version('2.11.1'), \\\n",
    "        'This demo requires Edge TPU version >= 2.11.1'\n",
    "from edgetpu.basic.basic_engine import BasicEngine\n",
    "# from edgetpu.utils import image_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoStream:\n",
    "    \"\"\"Camera object that controls video streaming from the Picamera\"\"\"\n",
    "    def __init__(self,resolution=(640,480),framerate=30):\n",
    "        # Initialize the PiCamera and the camera image stream\n",
    "        self.stream = cv2.VideoCapture(0)\n",
    "        ret = self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "        ret = self.stream.set(3,resolution[0])\n",
    "        ret = self.stream.set(4,resolution[1])\n",
    "            \n",
    "        # Read first frame from the stream\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "\t# Variable to control when the camera is stopped\n",
    "        self.stopped = False\n",
    "\n",
    "    def start(self):\n",
    "\t# Start the thread that reads frames from the video stream\n",
    "        Thread(target=self.update,args=()).start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # Keep looping indefinitely until the thread is stopped\n",
    "        while True:\n",
    "            # If the camera is stopped, stop the thread\n",
    "            if self.stopped:\n",
    "                # Close camera resources\n",
    "                self.stream.release()\n",
    "                return\n",
    "\n",
    "            # Otherwise, grab the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "    def read(self):\n",
    "\t# Return the most recent frame\n",
    "        return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "\t# Indicate that the camera and thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pose_TFLite_model\"\n",
    "LABELMAP_NAME = 'labelmap.txt'\n",
    "\n",
    "#18 fps\n",
    "resW, resH = '1280x720'.split('x')\n",
    "#60 fps\n",
    "# resW, resH = '640x480'.split('x')\n",
    "#110 fps\n",
    "# resW, resH = '480x352'.split('x')\n",
    "\n",
    "imW, imH = int(resW), int(resH)\n",
    "min_thresh = 0.2\n",
    "use_TPU = False\n",
    "_mirror = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_arrows(frame):\n",
    "    \"\"\"Show the direction vector output in the cv2 window\"\"\"\n",
    "    #cv2.putText(frame,\"Color:\", (0, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, 255, thickness=2)\n",
    "    cv2.arrowedLine(frame, (int(imW/2), int(imH/2)),\n",
    "                    (int(imW/2) + xoff, int(imH/2) - yoff),\n",
    "                    (0, 0, 255), 1)\n",
    "    return frame\n",
    "\n",
    "def DetectPosesInImage(img):\n",
    "    \"\"\"Detects poses in a given image.\n",
    "\n",
    "       For ideal results make sure the image fed to this function is close to the\n",
    "       expected input size - it is the caller's responsibility to resize the\n",
    "       image accordingly.\n",
    "\n",
    "    Args:\n",
    "      img: numpy array containing image\n",
    "    \"\"\"\n",
    "\n",
    "    # Extend or crop the input to match the input shape of the network.\n",
    "    if img.shape[0] < image_height or img.shape[1] < image_width:\n",
    "        img = np.pad(img, [[0, max(0, image_height - img.shape[0])],\n",
    "                           [0, max(0, image_width - img.shape[1])], [0, 0]],\n",
    "                     mode='constant')\n",
    "    img = img[0:image_height, 0:image_width]\n",
    "    assert (img.shape == tuple(_input_tensor_shape[1:]))\n",
    "\n",
    "    # Run the inference (API expects the data to be flattened)\n",
    "    return img\n",
    "\n",
    "def ParseOutput(output=None,KEYPOINTS=None):\n",
    "    global kpt_dict\n",
    "    inference_time, output = output\n",
    "    outputs = [output[i:j] for i, j in zip(_output_offsets, _output_offsets[1:])]\n",
    "    keypoints = outputs[0].reshape(-1, len(KEYPOINTS), 2)\n",
    "    keypoints = outputs[0].reshape(-1, len(KEYPOINTS), 2)\n",
    "    keypoint_scores = outputs[1].reshape(-1, len(KEYPOINTS))\n",
    "    pose_scores = outputs[2]\n",
    "    nposes = int(outputs[3][0])\n",
    "    pose_scores\n",
    "\n",
    "    # Convert the poses to a friendlier format of keypoints with associated\n",
    "    # scores.\n",
    "    poses = []\n",
    "#     for pose_i in range(nposes):\n",
    "    pose_i = 0\n",
    "#     print(pose_i)\n",
    "    kpt_dict = {}\n",
    "    for point_i, point in enumerate(keypoints[pose_i]):\n",
    "        keypoint = Keypoint(KEYPOINTS[point_i], list(point),\n",
    "                            keypoint_scores[pose_i, point_i])\n",
    "        if _mirror: keypoint.yx[1] = image_width - keypoint.yx[1]\n",
    "        kpt_dict[KEYPOINTS[point_i]] = keypoint\n",
    "        \n",
    "    return kpt_dict\n",
    "\n",
    "def Keypoint(k,yx,score):\n",
    "\n",
    "    return [k,yx,score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print in terminal\n",
    "import sys\n",
    "sys.stdout = open('/dev/stdout', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get path to current working directory\n",
    "CWD_PATH = os.getcwd()\n",
    "\n",
    "# Path to .tflite file, which contains the model that is used for object detection\n",
    "# PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)\n",
    "\n",
    "# Load the label map\n",
    "with open(PATH_TO_LABELS, 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize frame rate calculation\n",
    "frame_rate_calc = 1\n",
    "freq = cv2.getTickFrequency()\n",
    "\n",
    "xoff, yoff = 0,0\n",
    "# Initialize video stream\n",
    "videostream = VideoStream(resolution=(imW,imH),framerate=30).start()\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "model = \"models/posenet_mobilenet_v1_075_{}_{}_quant_decoder_edgetpu.tflite\".format(int(resH)+1, int(resW)+1)\n",
    "KEYPOINTS = labels\n",
    "engine = BasicEngine(model)#PoseEngine(model)\n",
    "input_shape = engine.get_input_tensor_shape()\n",
    "inference_size = (input_shape[2], input_shape[1])\n",
    "\n",
    "_input_tensor_shape = engine.get_input_tensor_shape()\n",
    "if (_input_tensor_shape.size != 4 or\n",
    "        _input_tensor_shape[3] != 3 or\n",
    "        _input_tensor_shape[0] != 1):\n",
    "    raise ValueError(\n",
    "        ('Image model should have input shape [1, height, width, 3]!'\n",
    "         ' This model has {}.'.format(self._input_tensor_shape)))\n",
    "_, image_height, image_width, image_depth = engine.get_input_tensor_shape()\n",
    "# The API returns all the output tensors flattened and concatenated. We\n",
    "# have to figure out the boundaries from the tensor shapes & sizes.\n",
    "offset = 0\n",
    "print(\"offset\",offset)\n",
    "_output_offsets = [0]\n",
    "for size in engine.get_all_output_tensors_sizes():\n",
    "    offset += size\n",
    "    _output_offsets.append(offset)\n",
    "    \n",
    "while True:\n",
    "\n",
    "    # Start timer (for calculating frame rate)\n",
    "    t1 = cv2.getTickCount()\n",
    "    \n",
    "    # Grab frame from video stream\n",
    "    frame = videostream.read()\n",
    "    width, height = 640,480\n",
    "    frame_resized = cv2.resize(frame, (width, height))\n",
    "    frame_resized = DetectPosesInImage(frame_resized)\n",
    "    keypoint_dict = ParseOutput(engine.run_inference(frame_resized.flatten()),KEYPOINTS)\n",
    "    # 5 draw keypoints\n",
    "    for idx,label in enumerate(labels):\n",
    "        if keypoint_dict[label][-1] >= min_thresh:\n",
    "            x = round((keypoint_dict[label][1][1]))\n",
    "            y = round((keypoint_dict[label][1][0]))\n",
    "            x = round((keypoint_dict[label][1][1]/width)*imW)\n",
    "            y = round((keypoint_dict[label][1][0]/height)*imH)\n",
    "            if 'right' in labels[idx]:\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (0,255,0), -1)\n",
    "            elif 'left' in labels[idx]:\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (0,0,255), -1)\n",
    "            else:\n",
    "                xoff, yoff = int(x-(imW/2)),int((imH/2)-y)\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (255,0,0), -1)\n",
    "    \n",
    "\n",
    "    draw_arrows(frame)\n",
    "    # Draw framerate in corner of frame\n",
    "    cv2.putText(frame,\n",
    "                'FPS: {0:.2f}'.format(frame_rate_calc),\n",
    "                (imW-200,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (255,255,0),\n",
    "                1,\n",
    "                cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    distance = 100\n",
    "    cmd = \"\"\n",
    "    # print(yoff)\n",
    "    tracking = True\n",
    "    if tracking:\n",
    "        if xoff < -distance and xoff>-imW/2:\n",
    "            cmd = \"counter_clockwise\"\n",
    "        elif xoff > distance and xoff<imW/2:\n",
    "            cmd = \"clockwise\"\n",
    "        elif yoff < -distance and yoff>-imH/2:\n",
    "            print(\"DOWNNNNN\",yoff)\n",
    "            cmd = \"down\"\n",
    "        elif yoff > distance and yoff<imH/2:\n",
    "            print(\"UPPPPPPPPPPPPPPP\",yoff)\n",
    "            # cmd = \"up\"\n",
    "        elif xoff==0 and yoff == 0:\n",
    "            print(\"ignore\")\n",
    "    \n",
    "    # All the results have been drawn on the frame, so it's time to display it.\n",
    "    cv2.imshow('posenet', frame)\n",
    "                    \n",
    "    # Calculate framerate\n",
    "    t2 = cv2.getTickCount()\n",
    "    time1 = (t2-t1)/freq\n",
    "    frame_rate_calc= 1/time1\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

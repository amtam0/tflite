{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from threading import Thread\n",
    "import importlib.util\n",
    "from PIL import Image\n",
    "import math\n",
    "use_TPU = False\n",
    "# # Import TensorFlow libraries\n",
    "# # If tensorflow is not installed, import interpreter from tflite_runtime, else import from regular tensorflow\n",
    "# # If using Coral Edge TPU, import the load_delegate library\n",
    "# pkg = importlib.util.find_spec('tensorflow')\n",
    "# if pkg is None:\n",
    "#     from tflite_runtime.interpreter import Interpreter\n",
    "#     if use_TPU:\n",
    "#         from tflite_runtime.interpreter import load_delegate\n",
    "# else:\n",
    "# #     from tensorflow.lite.python.interpreter import Interpreter\n",
    "#     if use_TPU:\n",
    "#         from tensorflow.lite.python.interpreter import load_delegate\n",
    "\n",
    "# # If using Edge TPU, assign filename for Edge TPU model\n",
    "# if use_TPU:\n",
    "#     # If user has specified the name of the .tflite file, use that name, otherwise use default 'edgetpu.tflite'\n",
    "#     if (GRAPH_NAME == 'detect.tflite'):\n",
    "#         GRAPH_NAME = 'edgetpu.tflite' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.lite.python.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoStream:\n",
    "    \"\"\"Camera object that controls video streaming from the Picamera\"\"\"\n",
    "    def __init__(self,resolution=(640,480),framerate=30):\n",
    "        # Initialize the PiCamera and the camera image stream\n",
    "        self.stream = cv2.VideoCapture(0)\n",
    "        ret = self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "        ret = self.stream.set(3,resolution[0])\n",
    "        ret = self.stream.set(4,resolution[1])\n",
    "            \n",
    "        # Read first frame from the stream\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "\t# Variable to control when the camera is stopped\n",
    "        self.stopped = False\n",
    "\n",
    "    def start(self):\n",
    "\t# Start the thread that reads frames from the video stream\n",
    "        Thread(target=self.update,args=()).start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # Keep looping indefinitely until the thread is stopped\n",
    "        while True:\n",
    "            # If the camera is stopped, stop the thread\n",
    "            if self.stopped:\n",
    "                # Close camera resources\n",
    "                self.stream.release()\n",
    "                return\n",
    "\n",
    "            # Otherwise, grab the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "    def read(self):\n",
    "\t# Return the most recent frame\n",
    "        return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "\t# Indicate that the camera and thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def argmax2d(inp_3d):\n",
    "    \"\"\"\n",
    "    Get the x,y positions of the heatmap of each part's argmax()\n",
    "    \"\"\"\n",
    "    heatmapPositions = np.zeros(shape=(17,2))\n",
    "    heatmapConf = np.zeros(shape=(17,1))\n",
    "    for i in range(17):\n",
    "        argmax_i =  np.unravel_index(inp_3d[:,:,i].argmax(), inp_3d[:,:,i].shape)\n",
    "        max_i =  inp_3d[:,:,i].max()\n",
    "        heatmapPositions[i,:] = argmax_i\n",
    "        heatmapConf[i,:] = max_i\n",
    "    return heatmapPositions,heatmapConf\n",
    "\n",
    "def get_offsetVector(heatmapPositions=None,offsets=None):\n",
    "    allArrays = np.zeros(shape=(17,2))\n",
    "    for idx,el in enumerate(heatmapPositions):\n",
    "#         print(el)\n",
    "        allArrays[idx,0] = offsets[int(el[0]),int(el[1]),idx]\n",
    "        allArrays[idx,1] = offsets[int(el[0]),int(el[1]),17+idx]\n",
    "    return allArrays\n",
    "\n",
    "def draw_arrows(frame):\n",
    "    \"\"\"Show the direction vector output in the cv2 window\"\"\"\n",
    "    #cv2.putText(frame,\"Color:\", (0, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, 255, thickness=2)\n",
    "    cv2.arrowedLine(frame, (int(imW/2), int(imH/2)),\n",
    "                    (int(imW/2) + xoff, int(imH/2) - yoff),\n",
    "                    (0, 0, 255), 1)\n",
    "    return frame\n",
    "    \n",
    "MODEL_NAME = \"Sample_TFLite_model\"\n",
    "MODEL_NAME = \"pose_TFLite_model\"\n",
    "GRAPH_NAME = 'detect.tflite'\n",
    "LABELMAP_NAME = 'labelmap.txt'\n",
    "min_conf_threshold = float(0.5)\n",
    "resW, resH = '1280x720'.split('x')\n",
    "imW, imH = int(resW), int(resH)\n",
    "min_thresh = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print in terminal\n",
    "import sys\n",
    "sys.stdout = open('/dev/stdout', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get path to current working directory\n",
    "CWD_PATH = os.getcwd()\n",
    "\n",
    "# Path to .tflite file, which contains the model that is used for object detection\n",
    "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)\n",
    "\n",
    "# Load the label map\n",
    "with open(PATH_TO_LABELS, 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Have to do a weird fix for label map if using the COCO \"starter model\" from\n",
    "# https://www.tensorflow.org/lite/models/object_detection/overview\n",
    "# First label is '???', which has to be removed.\n",
    "if labels[0] == '???':\n",
    "    del(labels[0])\n",
    "\n",
    "# Load the Tensorflow Lite model.\n",
    "# If using Edge TPU, use special load_delegate argument\n",
    "if use_TPU:\n",
    "    interpreter = Interpreter(model_path=PATH_TO_CKPT,\n",
    "                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\n",
    "    print(PATH_TO_CKPT)\n",
    "else:\n",
    "    interpreter = Interpreter(model_path=PATH_TO_CKPT)\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.invoke()\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "floating_model = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "input_mean = width/2\n",
    "input_std = width/2\n",
    "\n",
    "# Initialize frame rate calculation\n",
    "frame_rate_calc = 1\n",
    "freq = cv2.getTickFrequency()\n",
    "\n",
    "xoff, yoff = 0,0\n",
    "# Initialize video stream\n",
    "videostream = VideoStream(resolution=(imW,imH),framerate=30).start()\n",
    "time.sleep(1)\n",
    "#for frame1 in camera.capture_continuous(rawCapture, format=\"bgr\",use_video_port=True):\n",
    "while True:\n",
    "\n",
    "    # Start timer (for calculating frame rate)\n",
    "    t1 = cv2.getTickCount()\n",
    "    \n",
    "    # Grab frame from video stream\n",
    "    frame1 = videostream.read()\n",
    "\n",
    "    # Acquire frame and resize to expected shape [1xHxWx3]\n",
    "    frame = frame1.copy()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (width, height))\n",
    "    input_data = np.expand_dims(frame_resized, axis=0)\n",
    "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "    # Perform the actual detection by running the model with the image as input\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "    interpreter.invoke()\n",
    "    heatmapscores = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "    offsets = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "    # displacementFwd = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "    # displacementBwd = interpreter.get_tensor(output_details[3]['index'])[0]\n",
    "    \n",
    "    # define vectorized sigmoid\n",
    "    sigmoid_v = np.vectorize(sigmoid)\n",
    "    # 1 sigmoid\n",
    "    sigmoheatmapscores = sigmoid_v(heatmapscores)\n",
    "    # 2 argmax2d\n",
    "    heatmapPositions,heatmapConfidence = argmax2d(sigmoheatmapscores)\n",
    "    # 3 offsetVectors\n",
    "    offsetVectors = get_offsetVector(heatmapPositions,offsets)\n",
    "    # 4 keypointPositions\n",
    "    outputStride = 32\n",
    "    keypointPositions = heatmapPositions * outputStride + offsetVectors\n",
    "    # 5 draw keypoints\n",
    "    for idx,el in enumerate(heatmapConfidence):\n",
    "        if heatmapConfidence[idx][0] >= min_thresh:\n",
    "            x = round((keypointPositions[idx][1]/width)*imW)\n",
    "            y = round((keypointPositions[idx][0]/height)*imH)\n",
    "            if 'right' in labels[idx]:\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (0,255,0), -1)\n",
    "            elif 'left' in labels[idx]:\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (0,0,255), -1)\n",
    "            else:\n",
    "                xoff, yoff = int(x-(imW/2)),int((imH/2)-y)\n",
    "                cv2.circle(frame,(int(x),int(y)), 5, (255,0,0), -1)\n",
    "    draw_arrows(frame)\n",
    "    # Draw framerate in corner of frame\n",
    "    cv2.putText(frame,\n",
    "                'FPS: {0:.2f}'.format(frame_rate_calc),\n",
    "                (imW-200,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (255,255,0),\n",
    "                1,\n",
    "                cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    distance = 100\n",
    "    cmd = \"\"\n",
    "    # print(yoff)\n",
    "    tracking = True\n",
    "    if tracking:\n",
    "        if xoff < -distance and xoff>-imW/2:\n",
    "            cmd = \"counter_clockwise\"\n",
    "        elif xoff > distance and xoff<imW/2:\n",
    "            cmd = \"clockwise\"\n",
    "        elif yoff < -distance and yoff>-imH/2:\n",
    "            print(\"DOWNNNNN\",yoff)\n",
    "            cmd = \"down\"\n",
    "        elif yoff > distance and yoff<imH/2:\n",
    "            print(\"UPPPPPPPPPPPPPPP\",yoff)\n",
    "            # cmd = \"up\"\n",
    "        elif xoff==0 and yoff == 0:\n",
    "            print(\"ignore\")\n",
    "    \n",
    "    # All the results have been drawn on the frame, so it's time to display it.\n",
    "    cv2.imshow('Object detector', frame)\n",
    "                    \n",
    "    # Calculate framerate\n",
    "    t2 = cv2.getTickCount()\n",
    "    time1 = (t2-t1)/freq\n",
    "    frame_rate_calc= 1/time1\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for frame1 in camera.capture_continuous(rawCapture, format=\"bgr\",use_video_port=True):\n",
    "while True:\n",
    "\n",
    "    # Start timer (for calculating frame rate)\n",
    "    t1 = cv2.getTickCount()\n",
    "\n",
    "    # Grab frame from video stream\n",
    "    frame1 = videostream.read()\n",
    "\n",
    "    # Acquire frame and resize to expected shape [1xHxWx3]\n",
    "    frame = frame1.copy()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (width, height))\n",
    "    input_data = np.expand_dims(frame_resized, axis=0)\n",
    "\n",
    "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "    # Perform the actual detection by running the model with the image as input\n",
    "    interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve detection results\n",
    "    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n",
    "    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\n",
    "\n",
    "    # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "    for i in range(len(scores)):\n",
    "        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "\n",
    "            # Get bounding box coordinates and draw box\n",
    "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "            ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "            xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "            ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "            xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "            \n",
    "            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "            # Draw label\n",
    "            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "    # Draw framerate in corner of frame\n",
    "    cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)\n",
    "\n",
    "    # All the results have been drawn on the frame, so it's time to display it.\n",
    "    cv2.imshow('Object detector', frame)\n",
    "\n",
    "    # Calculate framerate\n",
    "    t2 = cv2.getTickCount()\n",
    "    time1 = (t2-t1)/freq\n",
    "    frame_rate_calc= 1/time1\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (py3env)",
   "language": "python",
   "name": "tflite1-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
